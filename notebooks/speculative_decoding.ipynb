{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "print(HF_TOKEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    # Set seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Ensure deterministic behavior on CUDA (GPU)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Franek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb8452a12844613ab2ad2557d24e289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Franek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Franek\\.cache\\huggingface\\hub\\models--gpt2-xl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aefdfdc985b4edfaf14f3453f737cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627d92cc17e0433eb4288f844746f3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba35d600c64e4681a5743b795c3535a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8adc8b514a493a9c6751335a10cb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674a52bb82394fe0ad55361da8b0595b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ffe9e19c48423a926d127eb5758a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"gpt2-xl\"\n",
    "checkpoint_assist = \"gpt2\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Select device (GPU or CPU)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_auth_token=HF_TOKEN)\n",
    "main_model = AutoModelForCausalLM.from_pretrained(checkpoint, use_auth_token=HF_TOKEN).to(device)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(checkpoint_assist, use_auth_token=HF_TOKEN).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time of vanilla: 17.47 seconds\n",
      "Output of vanilla: Hi, I'm a new member of the community. I'm a newbie to the forum, but I'm looking for a good place to start. I'm a newbie to the forum, but I'm looking for a good place to start.\n",
      "Speed of vanilla (averaged): 2.92 tokens per second\n",
      "17.46697473526001\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def vanilla_generation(model, tokenizer, prompt, max_tokens=50):\n",
    "    start = time.time()\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input = input.to(device)\n",
    "    out = model.generate(**input, max_new_tokens=max_tokens)\n",
    "    end = time.time()\n",
    "    tok = out.size(1)\n",
    "    print(f\"\\nTotal time of vanilla: {round((end-start), 2)} seconds\")\n",
    "    print(f\"Output of vanilla: {tokenizer.decode(out[0], skip_special_tokens=True)}\")\n",
    "    print(f\"Speed of vanilla (averaged): {round(tok/(end-start), 2)} tokens per second\")\n",
    "\n",
    "    return end-start\n",
    "    \n",
    "set_seed(42)\n",
    "\n",
    "sec = vanilla_generation(main_model, tokenizer, \"Hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_implementation(model, tokenizer, prompt, assistant_model, van_time, max_tokens=50):\n",
    "    start = time.time()\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input = input.to(device)\n",
    "    out = model.generate(**input, max_new_tokens=max_tokens, assistant_model=assistant_model)\n",
    "    end = time.time()\n",
    "    tok = out.size(1)\n",
    "    print(van_time)\n",
    "\n",
    "    print(f\"\\nTotal time of huggingface implementation: {end-start} seconds\" \n",
    "      f\"{f' ({(van_time)/(end-start):.2f}x speedup compared to vanilla!)' if van_time is not None else ''}\")\n",
    "    \n",
    "    print(f\"Output of huggingface implementation of spec dec: {tokenizer.decode(out[0], skip_special_tokens=True)}\")\n",
    "    print(f\"Speed of huggingface implementation of spec dec (averaged): {round(tok/(end-start), 2)} tokens per second\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main model output: Hi, I'm a student at the University\n",
      "Asistant model output: Hi. I'm sorry, but I'm\n"
     ]
    }
   ],
   "source": [
    "def check_models(assistant_model, main_model, tokenizer, prompt, max=8):\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input = input.to(device)\n",
    "    out_main = main_model.generate(**input, max_new_tokens=max)\n",
    "    out_assist = assistant_model.generate(**input, max_new_tokens=max)\n",
    "    print(f\"Main model output: {tokenizer.decode(out_main[0])}\")\n",
    "    print(f\"Asistant model output: {tokenizer.decode(out_assist[0])}\")\n",
    "check_models(assistant_model, main_model, tokenizer, \"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_decoding(tokenizer, model, assistant_model, prompt, van_time, max_len=50, speculative_len=8):\n",
    "    # Generating tokens we will speculate on:\n",
    "    start = time.time()\n",
    "    cur_len = 0\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input = input.to(device)\n",
    "     \n",
    "    while cur_len+1 < max_len:\n",
    "        candidate_input_ids = input[\"input_ids\"]\n",
    "        attn_mask = input[\"attention_mask\"]\n",
    "        # main_attn_mask = attn_mask\n",
    "        speculative_len = min(speculative_len, max_len-cur_len)\n",
    "        for i in range(speculative_len):\n",
    "            with torch.no_grad():\n",
    "                out = assistant_model(candidate_input_ids, attention_mask=attn_mask)\n",
    "                next_token = out.logits[:, -1, :].argmax(dim=-1)                \n",
    "                candidate_input_ids = torch.cat((candidate_input_ids, next_token[:, None]), dim=-1)\n",
    "                attn_mask = torch.cat((attn_mask, torch.ones_like(next_token[:, None])), dim=-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #verifying using main model:\n",
    "            assistant_ids = candidate_input_ids[:, -speculative_len:]\n",
    "            \n",
    "            out_logits = model(input_ids=candidate_input_ids, attention_mask=attn_mask)\n",
    "            last_logits = out_logits.logits[:, -speculative_len-1:, :]\n",
    "            main_ids = torch.argmax(last_logits, dim=-1)\n",
    "            main = torch.cat((input[\"input_ids\"], main_ids), dim=-1)\n",
    "            ass = torch.cat((input[\"input_ids\"], assistant_ids), dim=-1)\n",
    "            # print(f\"OUTPUT FROM THE MAIN MODEL wit prompt: {tokenizer.decode(main[0])}\")  \n",
    "            # print(f\"OUTPUT FROM THE ASSISTANT MODEL: {tokenizer.decode(ass[0])}\")  \n",
    "\n",
    "            match_mask = ~(assistant_ids == main_ids[:, :-1])\n",
    "\n",
    "            match_mask = match_mask.cumsum(dim=-1)\n",
    "            match_mask = match_mask < 1\n",
    "            n_matches = match_mask.sum().item()\n",
    "            valid_tokens = main_ids[:, :n_matches+1] # this is key, this ensures that even if n_matches are zero, we can always just come back to normal vanilla gen, because n_matches+1 is always true, because its still sampled from correct senstence it actually agreed with\n",
    "            attn = torch.ones_like(valid_tokens)\n",
    "            input[\"input_ids\"] = torch.cat((input[\"input_ids\"], valid_tokens), dim=-1)\n",
    "            input[\"attention_mask\"] = torch.cat((input[\"attention_mask\"], attn), dim=-1)\n",
    "\n",
    "\n",
    "            cur_len += n_matches+1\n",
    "            # print(f\"Current input after appending accepted: {tokenizer.decode(input['input_ids'][0])}\")\n",
    "            # print(input[\"input_ids\"].shape)\n",
    "            if n_matches+1 == speculative_len:\n",
    "                speculative_len+=2\n",
    "            else:\n",
    "                speculative_len = max(1, speculative_len-1)\n",
    "            \n",
    "    end = time.time()\n",
    "    tok = input[\"input_ids\"].size(1)\n",
    "    print(van_time)\n",
    "    print(f\"\\nTotal time of speculative decoding: {end-start} seconds\" \n",
    "      f\"{f' ({(van_time)/(end-start):.2f}x speedup compared to vanilla!)' if van_time is not None else ''}\")\n",
    "    print(f\"Speed of speculative decoding (averaged): {round(tok/(end-start), 2)} tokens per second\")\n",
    "    print(f\"Output of the model (speculative decoding): {tokenizer.decode(input['input_ids'][0])}\")\n",
    "    \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time of vanilla: 18.26 seconds\n",
      "Output of vanilla: Hi, I'm a new member of the community. I'm a newbie to the forum, but I'm looking for a good place to start. I'm a newbie to the forum, but I'm looking for a good place to start.\n",
      "Speed of vanilla (averaged): 2.79 tokens per second\n",
      "18.261565685272217\n",
      "\n",
      "Total time of huggingface implementation: 7.939016580581665 seconds (2.30x speedup compared to vanilla!)\n",
      "Output of huggingface implementation of spec dec: Hi, I'm a new member of the community. I'm a newbie to the forum, but I'm looking for a good place to start. I'm a newbie to the forum, but I'm looking for a good place to start.\n",
      "Speed of huggingface implementation of spec dec (averaged): 6.42 tokens per second\n",
      "18.261565685272217\n",
      "\n",
      "Total time of speculative decoding: 12.354034423828125 seconds (1.48x speedup compared to vanilla!)\n",
      "Speed of speculative decoding (averaged): 4.05 tokens per second\n",
      "Output of the model (speculative decoding): Hi, I'm a new member of the community. I'm a newbie to the forum, but I'm looking for a good place to start. I'm a newbie to the forum, but I'm looking for a good place to start\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    prompt = \"Hi\"\n",
    "    set_seed(42)\n",
    "    \n",
    "    van_time = vanilla_generation(main_model, tokenizer, prompt)\n",
    "\n",
    "    hf_implementation(main_model, tokenizer, prompt, assistant_model, van_time=van_time)\n",
    "\n",
    "    speculative_decoding(tokenizer, main_model, assistant_model, prompt, van_time=van_time)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
