{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "print(HF_TOKEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    # Set seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Ensure deterministic behavior on CUDA (GPU)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Franek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"gpt2\"\n",
    "checkpoint_assist = \"distilgpt2\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Select device (GPU or CPU)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_auth_token=HF_TOKEN)\n",
    "main_model = AutoModelForCausalLM.from_pretrained(checkpoint, use_auth_token=HF_TOKEN).to(device)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(checkpoint_assist, use_auth_token=HF_TOKEN).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, how are you doing?\n",
      "\n",
      "I'm doing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13299942016601562"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "def vanilla_generation(model, tokenizer, prompt, max_tokens=8):\n",
    "    start = time.time()\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input = input.to(device)\n",
    "    out = model.generate(**input, max_new_tokens=max_tokens)\n",
    "    end = time.time()\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    return end - start\n",
    "set_seed(42)\n",
    "\n",
    "vanilla_generation(main_model, tokenizer, \"Hi, how are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi how are\n",
      "4\n",
      "Hi how are\n",
      "3\n",
      "Hi how are\n",
      "2\n",
      "Hi how are\n",
      "1\n",
      "Hi how are\n",
      "0\n",
      "Hi how are you doing?\n",
      "\n",
      "I'm doing great.\n",
      "Speculative loop time: 0.5059974193572998\n",
      "Total time of vanilla if it was used: 0.20399999618530273\n",
      "Total time of function: 0.7099974155426025\n"
     ]
    }
   ],
   "source": [
    "def speculative_decoding(tokenizer, model, assistant_model, prompt, max_len=10, speculative_len=5, vocab_size=50257):\n",
    "    # Generating tokens we will speculate on:\n",
    "    start = time.time()\n",
    "    cur_len = 0\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input = input.to(device)\n",
    "   \n",
    "    while cur_len < max_len:\n",
    "        \n",
    "        \n",
    "        for _ in range(speculative_len):\n",
    "            with torch.no_grad():\n",
    "\n",
    "                out = assistant_model(**input)\n",
    "                next_token = out.logits[:, -1, :].argmax(dim=-1)\n",
    "                input[\"input_ids\"] = torch.cat((input[\"input_ids\"], next_token[:, None]), dim=-1)\n",
    "                input[\"attention_mask\"] = torch.cat((input[\"attention_mask\"], torch.ones_like(next_token[:, None])), dim=-1)\n",
    "        speculative_ids = input[\"input_ids\"][:, -speculative_len:]\n",
    "        with torch.no_grad():\n",
    "            #verifying using main model:\n",
    "            speculative_ids = input[\"input_ids\"][:, -speculative_len:]\n",
    "            if speculative_len > 0:\n",
    "                out_logits = model(**input)\n",
    "                last_logits = out_logits.logits[:, -speculative_len:, :]\n",
    "                verify_ids = torch.argmax(last_logits, dim=-1)\n",
    "                match_mask = ~(speculative_ids == verify_ids)\n",
    "                match_mask = match_mask.cumsum(dim=-1)\n",
    "                match_mask = match_mask < 1\n",
    "                n_matches = match_mask.sum().item()\n",
    "\n",
    "                if n_matches != speculative_len:\n",
    "                    input[\"input_ids\"] = input[\"input_ids\"][:, :-speculative_len + n_matches]\n",
    "                    input[\"attention_mask\"] = input[\"attention_mask\"][:, :-speculative_len + n_matches]\n",
    "\n",
    "                print(tokenizer.decode(input[\"input_ids\"][0]))\n",
    "                cur_len += n_matches\n",
    "\n",
    "                # Adjust speculative_len dynamically\n",
    "                if n_matches == speculative_len and speculative_len != 0:\n",
    "                    speculative_len += 2\n",
    "                elif n_matches < speculative_len / 5:\n",
    "                    speculative_len = max(0, speculative_len - 1)  # Ensure speculative_len is not negative\n",
    "\n",
    "                print(speculative_len)\n",
    "            else:\n",
    "                # Fallback to vanilla generation when speculative_len becomes 0\n",
    "                vanilla_time = vanilla_generation(model, tokenizer, prompt, max_tokens=max_len)\n",
    "                cur_len = max_len\n",
    "    end = time.time()\n",
    "    return end - start, end - start - vanilla_time, vanilla_time\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "prompt = \"Hi how are\"\n",
    "set_seed(42)\n",
    "total, spec, vanilla = speculative_decoding(tokenizer, main_model, assistant_model, prompt)\n",
    "print(f\"Speculative loop time: {spec}\")\n",
    "print(f\"Total time of vanilla if it was used: {vanilla}\")\n",
    "print(f\"Total time of function: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
