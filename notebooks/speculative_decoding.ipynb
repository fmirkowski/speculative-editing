{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "print(HF_TOKEN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    # Set seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Ensure deterministic behavior on CUDA (GPU)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Franek\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"gpt2\"\n",
    "checkpoint_assist = \"distilgpt2\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Select device (GPU or CPU)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_auth_token=HF_TOKEN)\n",
    "main_model = AutoModelForCausalLM.from_pretrained(checkpoint, use_auth_token=HF_TOKEN).to(device)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(checkpoint_assist, use_auth_token=HF_TOKEN).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi. I'm sorry, but I'm not sure if you're aware of this. I'm not sure if you're aware of this.\n",
      "\n",
      "I'm sorry, but I'm not sure if you're aware of this. I'm not sure if you're aware of this.\n",
      "\n",
      "I'm sorry, but I'm not sure if you're aware of this. I'm not sure\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7780017852783203"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "def vanilla_generation(model, tokenizer, prompt, max_tokens=79):\n",
    "    start = time.time()\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input = input.to(device)\n",
    "    out = model.generate(**input, max_new_tokens=max_tokens, assistant_model=assistant_model)\n",
    "    end = time.time()\n",
    "    print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    return end - start\n",
    "set_seed(42)\n",
    "\n",
    "vanilla_generation(main_model, tokenizer, \"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main model output: Hi, I'm a new user. I\n",
      "Asistant model output: Hi, I'm a student at the University\n"
     ]
    }
   ],
   "source": [
    "def check_models(assistant_model, main_model, tokenizer, prompt, max=8):\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input = input.to(device)\n",
    "    out_main = main_model.generate(**input, max_new_tokens=max)\n",
    "    out_assist = assistant_model.generate(**input, max_new_tokens=max)\n",
    "    print(f\"Main model output: {tokenizer.decode(out_main[0])}\")\n",
    "    print(f\"Asistant model output: {tokenizer.decode(out_assist[0])}\")\n",
    "check_models(assistant_model, main_model, tokenizer, \"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = tokenizer.eos_token_id\n",
    "main_model.config.pad_token_id = main_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of matches = 0\n",
      " Number of matches = 0\n",
      " Number of matches = 0\n",
      " Number of matches = 0\n",
      " Number of matches = 0\n",
      " Number of matches = 3\n",
      " Number of matches = 2\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 0\n",
      " Number of matches = 1\n",
      " Number of matches = 2\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 0\n",
      " Number of matches = 3\n",
      " Number of matches = 2\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      " Number of matches = 1\n",
      "Current input after appending accepted: Hi. I'm sorry, but I'm not sure if you're aware of this. I'm not sure if you're aware of this.\n",
      "\n",
      "I'm sorry, but I'm not sure if you're aware of this. I'm not sure if you're aware of this.\n",
      "\n",
      "I'm sorry, but I'm not sure if you're aware of this. I'm not sure if you're aware of this.\n",
      "\n",
      "I'm sorry, but I'm not sure if you're\n",
      "Total time of function: 3.232527256011963\n"
     ]
    }
   ],
   "source": [
    "def speculative_decoding(tokenizer, model, assistant_model, prompt, max_len=50, speculative_len=5, vocab_size=50257):\n",
    "    # Generating tokens we will speculate on:\n",
    "    start = time.time()\n",
    "    cur_len = 0\n",
    "    input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input = input.to(device)\n",
    "     \n",
    "    while cur_len < max_len:\n",
    "        candidate_input_ids = input[\"input_ids\"]\n",
    "        attn_mask = input[\"attention_mask\"]\n",
    "        # main_attn_mask = attn_mask\n",
    "        for i in range(speculative_len):\n",
    "            with torch.no_grad():\n",
    "                out = assistant_model(candidate_input_ids, attention_mask=attn_mask)\n",
    "                next_token = out.logits[:, -1, :].argmax(dim=-1)                \n",
    "                candidate_input_ids = torch.cat((candidate_input_ids, next_token[:, None]), dim=-1)\n",
    "                attn_mask = torch.cat((attn_mask, torch.ones_like(next_token[:, None])), dim=-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #verifying using main model:\n",
    "            assistant_ids = candidate_input_ids[:, -speculative_len:]\n",
    "            if speculative_len > 0:\n",
    "                out_logits = model(input_ids=candidate_input_ids, attention_mask=attn_mask)\n",
    "                last_logits = out_logits.logits[:, -speculative_len-1:, :]\n",
    "                main_ids = torch.argmax(last_logits, dim=-1)\n",
    "                main = torch.cat((input[\"input_ids\"], main_ids), dim=-1)\n",
    "                ass = torch.cat((input[\"input_ids\"], assistant_ids), dim=-1)\n",
    "                # print(f\"OUTPUT FROM THE MAIN MODEL wit prompt: {tokenizer.decode(main[0])}\")  \n",
    "                # print(f\"OUTPUT FROM THE ASSISTANT MODEL: {tokenizer.decode(ass[0])}\")  \n",
    "\n",
    "                match_mask = ~(assistant_ids == main_ids[:, :-1])\n",
    "\n",
    "                match_mask = match_mask.cumsum(dim=-1)\n",
    "                match_mask = match_mask < 1\n",
    "                n_matches = match_mask.sum().item()\n",
    "                valid_tokens = main_ids[:, :n_matches+1] # this is key, this ensures that even if n_matches are zero, we can always just come back to normal vanilla gen, because n_matches+1 is always true, because its still sampled from correct senstence it actually agreed with\n",
    "                attn = torch.ones_like(valid_tokens)\n",
    "                input[\"input_ids\"] = torch.cat((input[\"input_ids\"], valid_tokens), dim=-1)\n",
    "                input[\"attention_mask\"] = torch.cat((input[\"attention_mask\"], attn), dim=-1)\n",
    "\n",
    "\n",
    "                print(f\" Number of matches = {n_matches}\")\n",
    "                cur_len += n_matches\n",
    "                # print(f\"Current input after appending accepted: {tokenizer.decode(input['input_ids'][0])}\")\n",
    "                # print(input[\"input_ids\"].shape)\n",
    "                if n_matches+1 == speculative_len:\n",
    "                    speculative_len+=2\n",
    "                else:\n",
    "                    speculative_len = max(1, speculative_len-1)\n",
    "            # else:\n",
    "            #     # Fallback to vanilla generation when speculative_len becomes 0\n",
    "            #     vanilla_time = vanilla_generation(model, tokenizer, prompt, max_tokens=max_len)\n",
    "            #     cur_len = max_len\n",
    "    end = time.time()\n",
    "    print(f\"Current input after appending accepted: {tokenizer.decode(input['input_ids'][0])}\")\n",
    "    return end - start\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "prompt = \"Hi\"\n",
    "set_seed(42)\n",
    "total= speculative_decoding(tokenizer, main_model, assistant_model, prompt)\n",
    "# print(f\"Speculative loop time: {spec}\")\n",
    "# print(f\"Total time of vanilla if it was used: {vanilla}\")\n",
    "print(f\"Total time of function: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
